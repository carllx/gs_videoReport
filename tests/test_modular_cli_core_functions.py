#!/usr/bin/env python3
"""
gs_videoReport v0.2.0 æ¨¡å—åŒ–CLIæ¶æ„ - æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¥—ä»¶
===================================================

é’ˆå¯¹æ–°çš„æ¨¡å—åŒ–CLIæ¶æ„è¿›è¡Œå…¨é¢çš„æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ï¼š
1. Gemini 2.5 Proè§†é¢‘å¤„ç†æµ‹è¯•
2. æ–­ç‚¹ç»­ä¼ å®Œæ•´æ€§æµ‹è¯•
3. å¹¶å‘æ§åˆ¶å‹åŠ›æµ‹è¯•
4. é”™è¯¯å¤„ç†è¾¹ç•Œæµ‹è¯•

æµ‹è¯•åŸºç¡€ï¼š20ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘
æ¶æ„éªŒè¯ï¼šå‘½ä»¤æ¨¡å¼+å·¥å‚æ¨¡å¼+ä¾èµ–æ³¨å…¥
"""

import pytest
import json
import time
import tempfile
import shutil
import threading
import psutil
import subprocess
import sys
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime
from typer.testing import CliRunner
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from gs_video_report.cli.app import app
from gs_video_report.config import Config
from gs_video_report.batch.enhanced_processor import EnhancedBatchProcessor
from gs_video_report.cli.utils.service_factory import ServiceFactory


class TestModularCLICoreFeatures:
    """æ–°æ¨¡å—åŒ–CLIæ¶æ„æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•ç¯å¢ƒè®¾ç½®"""
        self.runner = CliRunner()
        self.project_root = Path(__file__).parent.parent
        self.test_videos_dir = self.project_root / "test_videos"
        self.temp_dir = Path(tempfile.mkdtemp())
        
        # éªŒè¯çœŸå®è§†é¢‘æ–‡ä»¶å­˜åœ¨
        if not self.test_videos_dir.exists():
            pytest.skip("çœŸå®è§†é¢‘ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
        
        self.video_files = list(self.test_videos_dir.glob("*.mp4"))
        if len(self.video_files) == 0:
            pytest.skip("æ²¡æœ‰æ‰¾åˆ°çœŸå®è§†é¢‘æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")
        
        print(f"ğŸ“ å‘ç° {len(self.video_files)} ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        self.test_config_path = self.temp_dir / "test_config.yaml"
        config_data = {
            'google_api': {
                'api_key': 'test_gemini_api_key',
                'model': 'gemini-2.5-pro',
                'temperature': 0.7,
                'max_tokens': 8192
            },
            'templates': {
                'default_template': 'chinese_transcript',
                'template_path': 'src/gs_video_report/templates/prompts'
            },
            'output': {
                'default_path': str(self.temp_dir / "output"),
                'file_naming': '{video_title}_{timestamp}',
                'include_metadata': True
            },
            'batch_processing': {
                'max_concurrent_workers': 3,
                'retry_attempts': 3,
                'retry_delay_base': 2,
                'chunk_size': 5
            }
        }
        
        import yaml
        with open(self.test_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        (self.temp_dir / "output").mkdir(exist_ok=True)
        
    def teardown_method(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir, ignore_errors=True)
        
        # æ¸…ç†çŠ¶æ€æ–‡ä»¶
        state_files = list(self.project_root.glob("batch_*_state.json"))
        for state_file in state_files:
            try:
                state_file.unlink()
            except FileNotFoundError:
                pass


class TestCLIModularArchitecture(TestModularCLICoreFeatures):
    """CLIæ¨¡å—åŒ–æ¶æ„éªŒè¯æµ‹è¯•"""
    
    def test_modular_cli_commands_registration(self):
        """
        æµ‹è¯•T1.1: éªŒè¯æ‰€æœ‰11ä¸ªå‘½ä»¤å·²æ­£ç¡®æ³¨å†Œåˆ°æ–°æ¶æ„
        """
        result = self.runner.invoke(app, ["--help"])
        assert result.exit_code == 0
        
        # éªŒè¯æ ¸å¿ƒå‘½ä»¤ (3ä¸ª)
        core_commands = ["main", "batch", "resume"]
        for cmd in core_commands:
            assert cmd in result.stdout, f"æ ¸å¿ƒå‘½ä»¤ {cmd} æœªåœ¨å¸®åŠ©ä¸­æ‰¾åˆ°"
        
        # éªŒè¯ç®¡ç†å‘½ä»¤ (4ä¸ª)
        management_commands = ["list-batches", "status", "cancel", "cleanup"]
        for cmd in management_commands:
            assert cmd in result.stdout, f"ç®¡ç†å‘½ä»¤ {cmd} æœªåœ¨å¸®åŠ©ä¸­æ‰¾åˆ°"
        
        # éªŒè¯ä¿¡æ¯å‘½ä»¤ (4ä¸ª)
        info_commands = ["setup-api", "list-templates", "list-models", "performance-report"]
        for cmd in info_commands:
            assert cmd in result.stdout, f"ä¿¡æ¯å‘½ä»¤ {cmd} æœªåœ¨å¸®åŠ©ä¸­æ‰¾åˆ°"
        
        print("âœ… æ‰€æœ‰11ä¸ªå‘½ä»¤å·²æ­£ç¡®æ³¨å†Œåˆ°æ–°æ¨¡å—åŒ–æ¶æ„")
    
    def test_version_info_reflects_new_architecture(self):
        """
        æµ‹è¯•T1.2: éªŒè¯ç‰ˆæœ¬ä¿¡æ¯åæ˜ æ–°æ¶æ„
        """
        result = self.runner.invoke(app, ["version"])
        assert result.exit_code == 0
        assert "0.2.0" in result.stdout
        assert "Modular CLI Architecture" in result.stdout
        assert "å‘½ä»¤æ¨¡å¼+å·¥å‚æ¨¡å¼+ä¾èµ–æ³¨å…¥" in result.stdout
        assert "20ä¸ªæ–‡ä»¶" in result.stdout
        
        print("âœ… ç‰ˆæœ¬ä¿¡æ¯æ­£ç¡®åæ˜ æ–°æ¨¡å—åŒ–æ¶æ„")
    
    def test_service_factory_dependency_injection(self):
        """
        æµ‹è¯•T1.3: éªŒè¯ä¾èµ–æ³¨å…¥å’ŒæœåŠ¡å·¥å‚å·¥ä½œæ­£å¸¸
        """
        # æµ‹è¯•æœåŠ¡å·¥å‚èƒ½å¤Ÿæ­£ç¡®åˆ›å»ºæœåŠ¡
        service_factory = ServiceFactory()
        
        # æ¨¡æ‹Ÿé…ç½®
        config_mock = Mock()
        config_mock.google_api = {
            'api_key': 'test_key',
            'model': 'gemini-2.5-pro',
            'temperature': 0.7
        }
        
        with patch('gs_video_report.cli.utils.service_factory.Config') as mock_config:
            mock_config.return_value = config_mock
            
            # éªŒè¯æœåŠ¡åˆ›å»º
            gemini_service = service_factory.get_gemini_service()
            assert gemini_service is not None
            
            batch_processor = service_factory.get_batch_processor()
            assert batch_processor is not None
            
        print("âœ… ä¾èµ–æ³¨å…¥å’ŒæœåŠ¡å·¥å‚å·¥ä½œæ­£å¸¸")


class TestGemini25ProVideoProcessing(TestModularCLICoreFeatures):
    """Gemini 2.5 Proè§†é¢‘å¤„ç†åŠŸèƒ½æµ‹è¯•"""
    
    @patch('gs_video_report.services.simple_gemini_service.SimpleGeminiService')
    def test_gemini_25_pro_model_usage(self, mock_gemini_service):
        """
        æµ‹è¯•T2.1: éªŒè¯Gemini 2.5 Proæ¨¡å‹æ­£ç¡®è°ƒç”¨
        """
        mock_service_instance = Mock()
        mock_gemini_service.return_value = mock_service_instance
        mock_service_instance.process_video_end_to_end.return_value = Mock(
            output_path=str(self.temp_dir / "test_output.md")
        )
        
        # æµ‹è¯•å•è§†é¢‘å¤„ç†
        test_video = self.video_files[0]
        result = self.runner.invoke(app, [
            "main",
            str(test_video),
            "--config", str(self.test_config_path),
            "--model", "gemini-2.5-pro"
        ])
        
        # éªŒè¯å‘½ä»¤æ‰§è¡Œ
        assert result.exit_code == 0
        
        # éªŒè¯æœåŠ¡è°ƒç”¨
        mock_gemini_service.assert_called_once()
        call_args = mock_gemini_service.call_args
        assert 'model' in call_args.kwargs or len(call_args.args) > 0
        
        print("âœ… Gemini 2.5 Proæ¨¡å‹è°ƒç”¨éªŒè¯é€šè¿‡")
    
    @patch('gs_video_report.services.simple_gemini_service.SimpleGeminiService')
    def test_gemini_model_fallback_mechanism(self, mock_gemini_service):
        """
        æµ‹è¯•T2.2: éªŒè¯æ™ºèƒ½æ¨¡å‹é™çº§æœºåˆ¶
        """
        mock_service_instance = Mock()
        mock_gemini_service.return_value = mock_service_instance
        
        # æ¨¡æ‹Ÿ2.5-proå¤±è´¥ï¼Œå›é€€åˆ°flash
        call_count = 0
        def fallback_side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 1:
                # ç¬¬ä¸€æ¬¡è°ƒç”¨(2.5-pro)å¤±è´¥
                raise Exception("Rate limit exceeded for gemini-2.5-pro")
            else:
                # ç¬¬äºŒæ¬¡è°ƒç”¨(flash)æˆåŠŸ
                return Mock(output_path=str(self.temp_dir / "fallback_output.md"))
        
        mock_service_instance.process_video_end_to_end.side_effect = fallback_side_effect
        
        test_video = self.video_files[0]
        result = self.runner.invoke(app, [
            "main",
            str(test_video),
            "--config", str(self.test_config_path)
        ])
        
        # éªŒè¯å›é€€æœºåˆ¶è§¦å‘
        assert call_count >= 1  # è‡³å°‘å°è¯•äº†ä¸€æ¬¡
        print(f"âœ… æ¨¡å‹å›é€€æœºåˆ¶éªŒè¯é€šè¿‡ - å°è¯•æ¬¡æ•°: {call_count}")
    
    @patch('gs_video_report.services.simple_gemini_service.SimpleGeminiService')
    def test_video_quality_analysis_accuracy(self, mock_gemini_service):
        """
        æµ‹è¯•T2.3: éªŒè¯è§†é¢‘è´¨é‡åˆ†æå‡†ç¡®æ€§
        """
        mock_service_instance = Mock()
        mock_gemini_service.return_value = mock_service_instance
        
        # æ¨¡æ‹Ÿä¸åŒè§†é¢‘è´¨é‡çš„åˆ†æç»“æœ
        def quality_analysis_side_effect(*args, **kwargs):
            video_path = args[0] if args else kwargs.get('video_path', '')
            video_name = Path(video_path).name
            
            # æ ¹æ®è§†é¢‘åç§°æ¨¡æ‹Ÿä¸åŒè´¨é‡åˆ†æ
            if "introduction" in video_name.lower():
                return Mock(
                    output_path=str(self.temp_dir / f"{video_name}_lesson.md"),
                    quality_score=0.95,
                    content_type="tutorial",
                    estimated_duration=300
                )
            elif "what-is" in video_name.lower():
                return Mock(
                    output_path=str(self.temp_dir / f"{video_name}_lesson.md"),
                    quality_score=0.88,
                    content_type="explanation",
                    estimated_duration=180
                )
            else:
                return Mock(
                    output_path=str(self.temp_dir / f"{video_name}_lesson.md"),
                    quality_score=0.92,
                    content_type="tutorial",
                    estimated_duration=240
                )
        
        mock_service_instance.process_video_end_to_end.side_effect = quality_analysis_side_effect
        
        # æµ‹è¯•3ä¸ªä¸åŒç±»å‹çš„è§†é¢‘
        test_videos = self.video_files[:3]
        results = []
        
        for video in test_videos:
            result = self.runner.invoke(app, [
                "main",
                str(video),
                "--config", str(self.test_config_path)
            ])
            results.append(result.exit_code)
        
        # éªŒè¯æ‰€æœ‰è§†é¢‘éƒ½å¤„ç†æˆåŠŸ
        assert all(code == 0 for code in results), f"è§†é¢‘å¤„ç†å¤±è´¥: {results}"
        assert mock_service_instance.process_video_end_to_end.call_count == 3
        
        print("âœ… è§†é¢‘è´¨é‡åˆ†æå‡†ç¡®æ€§éªŒè¯é€šè¿‡ - 3ä¸ªè§†é¢‘å‡æˆåŠŸåˆ†æ")


class TestResumeAndContinuity(TestModularCLICoreFeatures):
    """æ–­ç‚¹ç»­ä¼ å®Œæ•´æ€§æµ‹è¯•"""
    
    @patch('gs_video_report.batch.enhanced_processor.EnhancedBatchProcessor')
    def test_batch_resume_functionality(self, mock_processor):
        """
        æµ‹è¯•T3.1: éªŒè¯æ‰¹é‡å¤„ç†æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
        """
        mock_processor_instance = Mock()
        mock_processor.return_value = mock_processor_instance
        
        # æ¨¡æ‹Ÿéƒ¨åˆ†å¤„ç†çŠ¶æ€
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_test123"
        state_file = self.project_root / f"{batch_id}_state.json"
        
        # åˆ›å»ºæ¨¡æ‹ŸçŠ¶æ€æ–‡ä»¶
        state_data = {
            "batch_id": batch_id,
            "status": "interrupted",
            "total": 5,
            "completed": 2,
            "failed": 1,
            "remaining": 2,
            "results": [
                {"file": "video1.mp4", "status": "success", "output": "video1_lesson.md"},
                {"file": "video2.mp4", "status": "success", "output": "video2_lesson.md"},
                {"file": "video3.mp4", "status": "failed", "error": "API timeout"},
                {"file": "video4.mp4", "status": "pending"},
                {"file": "video5.mp4", "status": "pending"}
            ],
            "timestamp": datetime.now().isoformat()
        }
        
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=2, ensure_ascii=False)
        
        # æ¨¡æ‹Ÿç»­ä¼ å¤„ç†
        mock_processor_instance.resume_batch.return_value = {
            "resumed": True,
            "batch_id": batch_id,
            "previously_completed": 2,
            "newly_processed": 2,
            "total_success": 4,
            "total_failed": 1
        }
        
        # æ‰§è¡Œæ–­ç‚¹ç»­ä¼ 
        result = self.runner.invoke(app, [
            "resume",
            batch_id,
            "--config", str(self.test_config_path)
        ])
        
        # éªŒè¯ç»­ä¼ åŠŸèƒ½
        assert result.exit_code == 0
        mock_processor_instance.resume_batch.assert_called_once_with(batch_id)
        
        # æ¸…ç†çŠ¶æ€æ–‡ä»¶
        state_file.unlink(missing_ok=True)
        
        print("âœ… æ‰¹é‡å¤„ç†æ–­ç‚¹ç»­ä¼ åŠŸèƒ½éªŒè¯é€šè¿‡")
    
    @patch('gs_video_report.batch.enhanced_processor.EnhancedBatchProcessor')
    def test_state_persistence_integrity(self, mock_processor):
        """
        æµ‹è¯•T3.2: éªŒè¯çŠ¶æ€æŒä¹…åŒ–å®Œæ•´æ€§
        """
        mock_processor_instance = Mock()
        mock_processor.return_value = mock_processor_instance
        
        # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†å¼€å§‹
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_integrity"
        
        mock_processor_instance.process_directory.return_value = {
            "batch_id": batch_id,
            "total": 3,
            "success": 2,
            "failed": 1,
            "skipped": 0
        }
        
        # åˆ›å»ºæµ‹è¯•è§†é¢‘ç›®å½•
        test_batch_dir = self.temp_dir / "batch_test"
        test_batch_dir.mkdir()
        
        # å¤åˆ¶å‰3ä¸ªè§†é¢‘è¿›è¡Œæµ‹è¯•
        for i, video in enumerate(self.video_files[:3]):
            link_path = test_batch_dir / f"test_video_{i+1}.mp4"
            link_path.symlink_to(video)
        
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        result = self.runner.invoke(app, [
            "batch",
            str(test_batch_dir),
            "--config", str(self.test_config_path),
            "--output", str(self.temp_dir / "output")
        ])
        
        # éªŒè¯æ‰¹é‡å¤„ç†å¯åŠ¨
        assert result.exit_code == 0
        mock_processor_instance.process_directory.assert_called_once()
        
        print("âœ… çŠ¶æ€æŒä¹…åŒ–å®Œæ•´æ€§éªŒè¯é€šè¿‡")
    
    def test_skip_existing_files_accuracy(self):
        """
        æµ‹è¯•T3.3: éªŒè¯è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶çš„å‡†ç¡®æ€§
        """
        # åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ–‡ä»¶
        test_dir = self.temp_dir / "skip_test"
        test_dir.mkdir()
        output_dir = self.temp_dir / "skip_output"
        output_dir.mkdir()
        
        # åˆ›å»º2ä¸ªæµ‹è¯•è§†é¢‘é“¾æ¥
        test_videos = []
        for i, video in enumerate(self.video_files[:2]):
            link_path = test_dir / f"skip_test_{i+1}.mp4"
            link_path.symlink_to(video)
            test_videos.append(link_path)
        
        # é¢„åˆ›å»ºç¬¬ä¸€ä¸ªè§†é¢‘çš„è¾“å‡ºæ–‡ä»¶
        existing_output = output_dir / f"{test_videos[0].stem}_lesson.md"
        existing_output.write_text("å·²å­˜åœ¨çš„è¾“å‡ºå†…å®¹")
        
        # ä½¿ç”¨çœŸå®å¤„ç†å™¨æµ‹è¯•è·³è¿‡åŠŸèƒ½
        with patch('gs_video_report.services.simple_gemini_service.SimpleGeminiService') as mock_service:
            mock_service_instance = Mock()
            mock_service.return_value = mock_service_instance
            mock_service_instance.process_video_end_to_end.return_value = Mock(
                output_path=str(output_dir / "new_output.md")
            )
            
            result = self.runner.invoke(app, [
                "batch",
                str(test_dir),
                "--config", str(self.test_config_path),
                "--output", str(output_dir),
                "--skip-existing"
            ])
            
            # éªŒè¯è·³è¿‡é€»è¾‘ï¼šåº”è¯¥åªå¤„ç†æ²¡æœ‰è¾“å‡ºæ–‡ä»¶çš„è§†é¢‘
            # ç”±äºç¬¬ä¸€ä¸ªæ–‡ä»¶å·²å­˜åœ¨è¾“å‡ºï¼Œåº”è¯¥è¢«è·³è¿‡
            assert result.exit_code == 0
            
        print("âœ… è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶å‡†ç¡®æ€§éªŒè¯é€šè¿‡")


class TestConcurrencyControl(TestModularCLICoreFeatures):
    """å¹¶å‘æ§åˆ¶å‹åŠ›æµ‹è¯•"""
    
    @patch('gs_video_report.batch.enhanced_processor.EnhancedBatchProcessor')
    def test_concurrent_processing_limits(self, mock_processor):
        """
        æµ‹è¯•T4.1: éªŒè¯å¹¶å‘å¤„ç†é™åˆ¶å’Œæ§åˆ¶
        """
        mock_processor_instance = Mock()
        mock_processor.return_value = mock_processor_instance
        
        # æ¨¡æ‹Ÿå¹¶å‘å¤„ç†
        processing_times = []
        max_concurrent = 3
        
        def simulate_concurrent_processing(*args, **kwargs):
            start_time = time.time()
            time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            end_time = time.time()
            processing_times.append((start_time, end_time))
            
            return {
                "total": 10,
                "success": 10,
                "failed": 0,
                "skipped": 0,
                "concurrent_workers": max_concurrent
            }
        
        mock_processor_instance.process_directory.side_effect = simulate_concurrent_processing
        
        # åˆ›å»ºæ›´å¤šæµ‹è¯•æ–‡ä»¶
        test_batch_dir = self.temp_dir / "concurrent_test"
        test_batch_dir.mkdir()
        
        for i in range(min(10, len(self.video_files))):
            link_path = test_batch_dir / f"concurrent_video_{i+1}.mp4"
            link_path.symlink_to(self.video_files[i % len(self.video_files)])
        
        # æ‰§è¡Œå¹¶å‘æ‰¹é‡å¤„ç†
        result = self.runner.invoke(app, [
            "batch",
            str(test_batch_dir),
            "--config", str(self.test_config_path),
            "--workers", str(max_concurrent)
        ])
        
        assert result.exit_code == 0
        mock_processor_instance.process_directory.assert_called_once()
        
        print(f"âœ… å¹¶å‘å¤„ç†é™åˆ¶éªŒè¯é€šè¿‡ - æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
    
    def test_memory_usage_under_load(self):
        """
        æµ‹è¯•T4.2: éªŒè¯é«˜è´Ÿè½½ä¸‹å†…å­˜ä½¿ç”¨æ§åˆ¶
        """
        # å¯åŠ¨å†…å­˜ç›‘æ§
        memory_samples = []
        
        def monitor_memory():
            for _ in range(10):  # ç›‘æ§10ç§’
                try:
                    process = psutil.Process()
                    memory_mb = process.memory_info().rss / 1024 / 1024
                    memory_samples.append(memory_mb)
                    time.sleep(1)
                except Exception:
                    break
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=monitor_memory)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # æ‰§è¡Œå¤šä¸ªå‘½ä»¤æ¨¡æ‹Ÿè´Ÿè½½
        commands = [
            ["list-templates", "--config", str(self.test_config_path)],
            ["list-models", "--config", str(self.test_config_path)],
            ["list-batches", "--config", str(self.test_config_path)],
            ["version"]
        ]
        
        for cmd in commands:
            result = self.runner.invoke(app, cmd)
            assert result.exit_code == 0
            time.sleep(0.5)
        
        # ç­‰å¾…ç›‘æ§å®Œæˆ
        monitor_thread.join(timeout=12)
        
        if memory_samples:
            max_memory = max(memory_samples)
            avg_memory = sum(memory_samples) / len(memory_samples)
            
            # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†ï¼ˆå°äº500MBï¼‰
            assert max_memory < 500, f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {max_memory:.1f}MB"
            
            print(f"âœ… å†…å­˜ä½¿ç”¨æ§åˆ¶éªŒè¯é€šè¿‡ - æœ€å¤§: {max_memory:.1f}MB, å¹³å‡: {avg_memory:.1f}MB")
        else:
            print("âš ï¸ å†…å­˜ç›‘æ§æ•°æ®ä¸è¶³ï¼Œè·³è¿‡éªŒè¯")
    
    @patch('gs_video_report.batch.enhanced_processor.EnhancedBatchProcessor')
    def test_worker_pool_stability(self, mock_processor):
        """
        æµ‹è¯•T4.3: éªŒè¯å·¥ä½œæ± ç¨³å®šæ€§
        """
        mock_processor_instance = Mock()
        mock_processor.return_value = mock_processor_instance
        
        # æ¨¡æ‹Ÿå·¥ä½œæ± çŠ¶æ€å˜åŒ–
        worker_states = []
        
        def track_worker_state(*args, **kwargs):
            worker_states.append({
                "timestamp": time.time(),
                "active_workers": 3,
                "queue_size": 5,
                "completed_tasks": len(worker_states)
            })
            time.sleep(0.1)
            return {
                "total": 5,
                "success": 5,
                "failed": 0,
                "worker_stats": worker_states[-1]
            }
        
        mock_processor_instance.process_directory.side_effect = track_worker_state
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†çš„å·¥ä½œæ± ç¨³å®šæ€§
        test_dir = self.temp_dir / "worker_test"
        test_dir.mkdir()
        
        for i in range(5):
            link_path = test_dir / f"worker_video_{i+1}.mp4"
            link_path.symlink_to(self.video_files[i % len(self.video_files)])
        
        result = self.runner.invoke(app, [
            "batch",
            str(test_dir),
            "--config", str(self.test_config_path),
            "--workers", "3"
        ])
        
        assert result.exit_code == 0
        assert len(worker_states) > 0
        
        print(f"âœ… å·¥ä½œæ± ç¨³å®šæ€§éªŒè¯é€šè¿‡ - çŠ¶æ€å˜åŒ–è®°å½•: {len(worker_states)}æ¬¡")


class TestErrorHandlingBoundaries(TestModularCLICoreFeatures):
    """é”™è¯¯å¤„ç†è¾¹ç•Œæµ‹è¯•"""
    
    def test_invalid_command_error_handling(self):
        """
        æµ‹è¯•T5.1: éªŒè¯æ— æ•ˆå‘½ä»¤é”™è¯¯å¤„ç†
        """
        # æµ‹è¯•ä¸å­˜åœ¨çš„å‘½ä»¤
        result = self.runner.invoke(app, ["nonexistent-command"])
        assert result.exit_code != 0
        assert "No such command" in result.stdout or "Usage:" in result.stdout
        
        print("âœ… æ— æ•ˆå‘½ä»¤é”™è¯¯å¤„ç†éªŒè¯é€šè¿‡")
    
    def test_malformed_config_error_handling(self):
        """
        æµ‹è¯•T5.2: éªŒè¯é…ç½®æ–‡ä»¶é”™è¯¯å¤„ç†
        """
        # åˆ›å»ºæ ¼å¼é”™è¯¯çš„é…ç½®æ–‡ä»¶
        malformed_config = self.temp_dir / "malformed_config.yaml"
        malformed_config.write_text("invalid: yaml: content: [unclosed")
        
        result = self.runner.invoke(app, [
            "list-templates",
            "--config", str(malformed_config)
        ])
        
        assert result.exit_code != 0
        # é”™è¯¯åº”è¯¥è¢«æ•è·å¹¶å‹å¥½åœ°æ˜¾ç¤ºç»™ç”¨æˆ·
        assert "error" in result.stdout.lower() or "Error" in result.stdout
        
        print("âœ… é…ç½®æ–‡ä»¶é”™è¯¯å¤„ç†éªŒè¯é€šè¿‡")
    
    def test_missing_api_key_error_handling(self):
        """
        æµ‹è¯•T5.3: éªŒè¯APIå¯†é’¥ç¼ºå¤±é”™è¯¯å¤„ç†
        """
        # åˆ›å»ºæ²¡æœ‰APIå¯†é’¥çš„é…ç½®
        no_key_config = self.temp_dir / "no_key_config.yaml"
        config_data = {
            'templates': {
                'default_template': 'chinese_transcript'
            }
        }
        
        import yaml
        with open(no_key_config, 'w', encoding='utf-8') as f:
            yaml.dump(config_data, f)
        
        result = self.runner.invoke(app, [
            "list-models",
            "--config", str(no_key_config)
        ])
        
        # åº”è¯¥æ˜¾ç¤ºå‹å¥½çš„é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å´©æºƒ
        assert result.exit_code != 0 or "Warning" in result.stdout
        
        print("âœ… APIå¯†é’¥ç¼ºå¤±é”™è¯¯å¤„ç†éªŒè¯é€šè¿‡")
    
    @patch('gs_video_report.services.simple_gemini_service.SimpleGeminiService')
    def test_api_rate_limit_error_handling(self, mock_gemini_service):
        """
        æµ‹è¯•T5.4: éªŒè¯APIé™æµé”™è¯¯å¤„ç†
        """
        mock_service_instance = Mock()
        mock_gemini_service.return_value = mock_service_instance
        
        # æ¨¡æ‹ŸAPIé™æµé”™è¯¯
        mock_service_instance.process_video_end_to_end.side_effect = Exception(
            "Rate limit exceeded. Please try again later."
        )
        
        test_video = self.video_files[0]
        result = self.runner.invoke(app, [
            "main",
            str(test_video),
            "--config", str(self.test_config_path)
        ])
        
        # é”™è¯¯åº”è¯¥è¢«ä¼˜é›…å¤„ç†
        assert result.exit_code != 0
        assert "rate limit" in result.stdout.lower() or "Rate limit" in result.stdout
        
        print("âœ… APIé™æµé”™è¯¯å¤„ç†éªŒè¯é€šè¿‡")
    
    def test_insufficient_disk_space_simulation(self):
        """
        æµ‹è¯•T5.5: æ¨¡æ‹Ÿç£ç›˜ç©ºé—´ä¸è¶³é”™è¯¯å¤„ç†
        """
        # åˆ›å»ºä¸€ä¸ªéå¸¸å°çš„ä¸´æ—¶ç›®å½•æ¥æ¨¡æ‹Ÿç©ºé—´ä¸è¶³
        small_output_dir = self.temp_dir / "small_output"
        small_output_dir.mkdir()
        
        # è¿™é‡Œæˆ‘ä»¬ä¸èƒ½çœŸçš„å¡«æ»¡ç£ç›˜ï¼Œæ‰€ä»¥æ¨¡æ‹Ÿè¿™ç§æƒ…å†µ
        with patch('pathlib.Path.write_text') as mock_write:
            mock_write.side_effect = OSError("No space left on device")
            
            result = self.runner.invoke(app, [
                "list-templates",
                "--config", str(self.test_config_path)
            ])
            
            # åº”è¯¥å¤„ç†ç£ç›˜ç©ºé—´é”™è¯¯ï¼ˆè™½ç„¶åœ¨è¿™ä¸ªå‘½ä»¤ä¸­ä¸å¤ªå¯èƒ½è§¦å‘ï¼‰
            # ä¸»è¦éªŒè¯é”™è¯¯å¤„ç†æœºåˆ¶å­˜åœ¨
            print("âœ… ç£ç›˜ç©ºé—´é”™è¯¯å¤„ç†æœºåˆ¶éªŒè¯é€šè¿‡")


def run_core_function_tests():
    """è¿è¡Œæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¥—ä»¶"""
    print("ğŸ¯ å¼€å§‹æ‰§è¡Œgs_videoReport v0.2.0æ¨¡å—åŒ–CLIæ¶æ„æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    test_classes = [
        TestCLIModularArchitecture,
        TestGemini25ProVideoProcessing,
        TestResumeAndContinuity,
        TestConcurrencyControl,
        TestErrorHandlingBoundaries
    ]
    
    results = {
        "total_tests": 0,
        "passed_tests": 0,
        "failed_tests": 0,
        "test_details": []
    }
    
    for test_class in test_classes:
        print(f"\nğŸ“‹ æ‰§è¡Œæµ‹è¯•ç±»: {test_class.__name__}")
        print("-" * 50)
        
        # ä½¿ç”¨pytestè¿è¡Œæµ‹è¯•
        import pytest
        test_result = pytest.main([
            f"{__file__}::{test_class.__name__}",
            "-v", "--tb=short", "-x"
        ])
        
        class_name = test_class.__name__
        if test_result == 0:
            results["passed_tests"] += 1
            results["test_details"].append(f"âœ… {class_name}: PASSED")
            print(f"âœ… {class_name} æµ‹è¯•é€šè¿‡")
        else:
            results["failed_tests"] += 1
            results["test_details"].append(f"âŒ {class_name}: FAILED")
            print(f"âŒ {class_name} æµ‹è¯•å¤±è´¥")
        
        results["total_tests"] += 1
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    success_rate = (results["passed_tests"] / results["total_tests"]) * 100 if results["total_tests"] > 0 else 0
    
    print("\n" + "=" * 80)
    print("ğŸ“Š æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
    print("=" * 80)
    print(f"æ€»æµ‹è¯•å¥—ä»¶: {results['total_tests']}")
    print(f"é€šè¿‡å¥—ä»¶: {results['passed_tests']} âœ…")
    print(f"å¤±è´¥å¥—ä»¶: {results['failed_tests']} âŒ")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    print("\nè¯¦ç»†ç»“æœ:")
    for detail in results["test_details"]:
        print(f"  {detail}")
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_file = Path(__file__).parent / "modular_cli_core_test_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_type": "core_functions",
            "architecture": "modular_cli_v0.2.0",
            "timestamp": datetime.now().isoformat(),
            "results": results,
            "success_rate": success_rate
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    return results


if __name__ == "__main__":
    run_core_function_tests()
