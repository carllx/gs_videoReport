#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†åŠŸèƒ½ - QAæµ‹è¯•æ‰§è¡Œå™¨
å¿«é€Ÿæ‰§è¡Œå…³é”®æµ‹è¯•ç”¨ä¾‹å¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
"""

import subprocess
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class BatchQATestRunner:
    """æ‰¹é‡å¤„ç†QAæµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_results = []
        self.start_time = None
        self.end_time = None
    
    def run_test_suite(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„QAæµ‹è¯•å¥—ä»¶"""
        
        print("ðŸ” QA Agent - å¼€å§‹æ‰§è¡Œæ‰¹é‡å¤„ç†åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        self.start_time = time.time()
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹å¥—ä»¶
        test_cases = [
            {
                'name': 'é”™è¯¯éš”ç¦»æœºåˆ¶éªŒè¯',
                'module': 'test_batch_error_handling.py',
                'class': 'TestErrorHandlingAndRetry',
                'method': 'test_error_isolation_mechanism',
                'priority': 'HIGH',
                'category': 'å¯é æ€§'
            },
            {
                'name': 'ç½‘ç»œé”™è¯¯é‡è¯•æœºåˆ¶',
                'module': 'test_batch_error_handling.py', 
                'class': 'TestErrorHandlingAndRetry',
                'method': 'test_network_error_retry_mechanism',
                'priority': 'HIGH',
                'category': 'é”™è¯¯å¤„ç†'
            },
            {
                'name': 'æ°¸ä¹…é”™è¯¯è¯†åˆ«',
                'module': 'test_batch_error_handling.py',
                'class': 'TestErrorHandlingAndRetry', 
                'method': 'test_permanent_error_no_retry',
                'priority': 'HIGH',
                'category': 'æ™ºèƒ½é‡è¯•'
            },
            {
                'name': 'çŠ¶æ€æŒä¹…åŒ–ä¸­æ–­æµ‹è¯•',
                'module': 'test_batch_error_handling.py',
                'class': 'TestFileUploadInterruption',
                'method': 'test_state_persistence_on_interruption', 
                'priority': 'HIGH',
                'category': 'ä¸­æ–­æ¢å¤'
            },
            {
                'name': 'å°æ‰¹é‡æ€§èƒ½åŸºçº¿',
                'module': 'test_batch_performance.py',
                'class': 'TestBatchPerformanceBenchmark',
                'method': 'test_c1_small_batch_performance_baseline',
                'priority': 'MEDIUM',
                'category': 'æ€§èƒ½åŸºå‡†'
            },
            {
                'name': 'ä¸­ç­‰æ‰¹é‡æ€§èƒ½æµ‹è¯•',
                'module': 'test_batch_performance.py',
                'class': 'TestBatchPerformanceBenchmark', 
                'method': 'test_c2_medium_batch_performance',
                'priority': 'MEDIUM',
                'category': 'æ€§èƒ½éªŒè¯'
            },
            {
                'name': 'çœŸå®žè§†é¢‘æ€§èƒ½æµ‹è¯•',
                'module': 'test_batch_performance.py',
                'class': 'TestRealWorldPerformance',
                'method': 'test_real_figma_videos_performance',
                'priority': 'HIGH', 
                'category': 'çœŸå®žåœºæ™¯'
            }
        ]
        
        # æ‰§è¡Œæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
        for test_case in test_cases:
            result = self._run_single_test(test_case)
            self.test_results.append(result)
        
        self.end_time = time.time()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self._generate_test_report()
        
        # è¾“å‡ºç»“æžœæ‘˜è¦
        self._print_summary()
        
        return report
    
    def _run_single_test(self, test_case: Dict[str, str]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        
        test_identifier = f"{test_case['module']}::{test_case['class']}::{test_case['method']}"
        
        print(f"\nðŸ§ª æ‰§è¡Œ: {test_case['name']}")
        print(f"   ðŸ“‹ ç±»åˆ«: {test_case['category']} | ä¼˜å…ˆçº§: {test_case['priority']}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œpytestå‘½ä»¤
            result = subprocess.run([
                sys.executable, "-m", "pytest",
                f"tests/{test_identifier}",
                "-v", "--tb=short", "--no-header"
            ], 
            cwd=self.project_root,
            capture_output=True, 
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # è§£æžæµ‹è¯•ç»“æžœ
            if result.returncode == 0:
                status = "PASS"
                print(f"   âœ… é€šè¿‡ ({duration:.2f}s)")
            else:
                status = "FAIL"
                print(f"   âŒ å¤±è´¥ ({duration:.2f}s)")
                if result.stderr:
                    print(f"   ðŸ” é”™è¯¯: {result.stderr.strip()[:200]}")
            
            return {
                'test_case': test_case,
                'status': status,
                'duration_seconds': duration,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'return_code': result.returncode
            }
            
        except subprocess.TimeoutExpired:
            print(f"   â±ï¸ è¶…æ—¶ (>300s)")
            return {
                'test_case': test_case,
                'status': 'TIMEOUT', 
                'duration_seconds': 300,
                'stdout': '',
                'stderr': 'Test execution timeout',
                'return_code': -1
            }
        except Exception as e:
            print(f"   ðŸ’¥ å¼‚å¸¸: {str(e)}")
            return {
                'test_case': test_case,
                'status': 'ERROR',
                'duration_seconds': 0,
                'stdout': '',
                'stderr': str(e),
                'return_code': -1
            }
    
    def _generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
        
        total_duration = self.end_time - self.start_time
        
        # ç»Ÿè®¡ç»“æžœ
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['status'] == 'PASS'])
        failed_tests = len([r for r in self.test_results if r['status'] == 'FAIL'])
        error_tests = len([r for r in self.test_results if r['status'] in ['ERROR', 'TIMEOUT']])
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        category_stats = {}
        priority_stats = {}
        
        for result in self.test_results:
            category = result['test_case']['category']
            priority = result['test_case']['priority']
            status = result['status']
            
            if category not in category_stats:
                category_stats[category] = {'total': 0, 'pass': 0, 'fail': 0}
            category_stats[category]['total'] += 1
            if status == 'PASS':
                category_stats[category]['pass'] += 1
            elif status == 'FAIL':
                category_stats[category]['fail'] += 1
            
            if priority not in priority_stats:
                priority_stats[priority] = {'total': 0, 'pass': 0, 'fail': 0}
            priority_stats[priority]['total'] += 1
            if status == 'PASS':
                priority_stats[priority]['pass'] += 1
            elif status == 'FAIL':
                priority_stats[priority]['fail'] += 1
        
        # æž„å»ºå®Œæ•´æŠ¥å‘Š
        report = {
            'qa_test_execution_report': {
                'metadata': {
                    'agent_role': '@qa.mdc',
                    'test_suite': 'batch_processing_qa',
                    'execution_timestamp': datetime.now().isoformat(),
                    'total_duration_seconds': total_duration,
                    'project_root': str(self.project_root)
                },
                'summary': {
                    'total_tests': total_tests,
                    'passed': passed_tests,
                    'failed': failed_tests,
                    'errors': error_tests,
                    'pass_rate': passed_tests / total_tests if total_tests > 0 else 0,
                    'overall_status': 'PASS' if failed_tests == 0 and error_tests == 0 else 'FAIL'
                },
                'category_breakdown': category_stats,
                'priority_breakdown': priority_stats,
                'test_results': self.test_results,
                'recommendations': self._generate_recommendations()
            }
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = self.project_root / "tests" / f"qa_batch_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nðŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """åŸºäºŽæµ‹è¯•ç»“æžœç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        failed_tests = [r for r in self.test_results if r['status'] != 'PASS']
        
        if not failed_tests:
            recommendations.append("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ‰¹é‡å¤„ç†åŠŸèƒ½è´¨é‡è¾¾æ ‡")
            recommendations.append("ðŸš€ å¯ä»¥æŽ¨è¿›åˆ°ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²")
        else:
            recommendations.append("âš ï¸ å‘çŽ°æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•å’Œä¿®å¤")
            
            # åˆ†æžå¤±è´¥æ¨¡å¼
            error_patterns = {}
            for result in failed_tests:
                category = result['test_case']['category']
                if category not in error_patterns:
                    error_patterns[category] = 0
                error_patterns[category] += 1
            
            for category, count in error_patterns.items():
                recommendations.append(f"ðŸ” {category} ç±»åˆ«æœ‰ {count} ä¸ªå¤±è´¥æµ‹è¯•ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨")
        
        # æ€§èƒ½ç›¸å…³å»ºè®®
        performance_tests = [r for r in self.test_results if 'performance' in r['test_case']['name'].lower()]
        slow_tests = [r for r in performance_tests if r['duration_seconds'] > 60]
        
        if slow_tests:
            recommendations.append(f"â±ï¸ å‘çŽ° {len(slow_tests)} ä¸ªè€—æ—¶è¾ƒé•¿çš„æ€§èƒ½æµ‹è¯•ï¼Œå»ºè®®ä¼˜åŒ–")
        
        return recommendations
    
    def _print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æžœæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ðŸ“Š QAæµ‹è¯•æ‰§è¡Œæ‘˜è¦")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['status'] == 'PASS'])
        failed_tests = len([r for r in self.test_results if r['status'] == 'FAIL'])
        
        print(f"ðŸ“ˆ æ€»è®¡æµ‹è¯•: {total_tests}")
        print(f"âœ… é€šè¿‡: {passed_tests}")
        print(f"âŒ å¤±è´¥: {failed_tests}")
        print(f"ðŸ“Š é€šè¿‡çŽ‡: {passed_tests/total_tests:.1%}")
        print(f"â±ï¸ æ€»è€—æ—¶: {self.end_time - self.start_time:.2f}ç§’")
        
        # å…³é”®æµ‹è¯•çŠ¶æ€
        critical_tests = [r for r in self.test_results if r['test_case']['priority'] == 'HIGH']
        critical_passed = len([r for r in critical_tests if r['status'] == 'PASS'])
        
        print(f"\nðŸŽ¯ å…³é”®æµ‹è¯• (HIGHä¼˜å…ˆçº§): {critical_passed}/{len(critical_tests)} é€šè¿‡")
        
        if failed_tests == 0:
            print(f"\nðŸŽ‰ QAéªŒè¯å®Œæˆï¼æ‰¹é‡å¤„ç†åŠŸèƒ½è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥äº¤ä»˜ä½¿ç”¨ã€‚")
        else:
            print(f"\nâš ï¸ å‘çŽ° {failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦å¼€å‘å›¢é˜Ÿè¿›ä¸€æ­¥ä¿®å¤ã€‚")
    
    def run_quick_smoke_test(self):
        """æ‰§è¡Œå¿«é€Ÿå†’çƒŸæµ‹è¯• - éªŒè¯æ ¸å¿ƒåŠŸèƒ½"""
        print("ðŸš€ æ‰§è¡Œå¿«é€Ÿå†’çƒŸæµ‹è¯•...")
        
        # åªæ‰§è¡Œæœ€å…³é”®çš„æµ‹è¯•
        critical_tests = [
            'test_batch_error_handling.py::TestErrorHandlingAndRetry::test_error_isolation_mechanism'
        ]
        
        for test in critical_tests:
            print(f"ðŸ§ª æ‰§è¡Œ: {test}")
            result = subprocess.run([
                sys.executable, "-m", "pytest", f"tests/{test}", "-v"
            ], cwd=self.project_root)
            
            if result.returncode == 0:
                print("âœ… å†’çƒŸæµ‹è¯•é€šè¿‡ - æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")
                return True
            else:
                print("âŒ å†’çƒŸæµ‹è¯•å¤±è´¥ - éœ€è¦ä¿®å¤æ ¸å¿ƒé—®é¢˜")
                return False


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    runner = BatchQATestRunner()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--smoke":
        # å¿«é€Ÿå†’çƒŸæµ‹è¯•
        return runner.run_quick_smoke_test()
    else:
        # å®Œæ•´æµ‹è¯•å¥—ä»¶
        runner.run_test_suite()
        return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
