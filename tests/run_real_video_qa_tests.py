#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†åŠŸèƒ½ - åŸºäºçœŸå®è§†é¢‘çš„QAæµ‹è¯•æ‰§è¡Œå™¨
ä½¿ç”¨test_videosç›®å½•ä¸­çš„20ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘è¿›è¡Œå®Œæ•´éªŒè¯
"""

import subprocess
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class RealVideoQATestRunner:
    """åŸºäºçœŸå®è§†é¢‘çš„QAæµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_videos_dir = self.project_root / "test_videos"
        self.test_results = []
        self.start_time = None
        self.end_time = None
        
        # éªŒè¯çœŸå®è§†é¢‘æ–‡ä»¶
        self._validate_real_videos()
    
    def _validate_real_videos(self):
        """éªŒè¯çœŸå®è§†é¢‘æ–‡ä»¶çš„å­˜åœ¨æ€§"""
        if not self.test_videos_dir.exists():
            print("âŒ test_videosç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®è§†é¢‘æµ‹è¯•")
            sys.exit(1)
        
        video_files = list(self.test_videos_dir.glob("*.mp4"))
        if len(video_files) != 20:
            print(f"âš ï¸ æœŸæœ›20ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå®é™…æ‰¾åˆ°{len(video_files)}ä¸ª")
        
        print(f"âœ… æ‰¾åˆ° {len(video_files)} ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘æ–‡ä»¶")
        return video_files
    
    def run_comprehensive_qa_suite(self) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºäºçœŸå®è§†é¢‘çš„å®Œæ•´QAæµ‹è¯•å¥—ä»¶"""
        
        print("ğŸ¬ QA Agent - å¼€å§‹æ‰§è¡ŒåŸºäºçœŸå®Figmaè§†é¢‘çš„æ‰¹é‡å¤„ç†æµ‹è¯•")
        print("=" * 70)
        
        self.start_time = time.time()
        
        # å®šä¹‰çœŸå®è§†é¢‘æµ‹è¯•ç”¨ä¾‹å¥—ä»¶
        real_video_test_cases = [
            {
                'name': 'çœŸå®è§†é¢‘æ–‡ä»¶éªŒè¯',
                'module': 'test_batch_real_videos.py',
                'class': 'TestBatchProcessingWithRealVideos',
                'method': 'test_real_video_files_validation',
                'priority': 'HIGH',
                'category': 'åŸºç¡€éªŒè¯',
                'description': 'éªŒè¯20ä¸ªFigmaæ•™ç¨‹è§†é¢‘æ–‡ä»¶çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§'
            },
            {
                'name': 'çœŸå®è§†é¢‘é”™è¯¯éš”ç¦»æœºåˆ¶',
                'module': 'test_batch_real_videos.py',
                'class': 'TestBatchProcessingWithRealVideos', 
                'method': 'test_real_video_error_isolation',
                'priority': 'HIGH',
                'category': 'å¯é æ€§æµ‹è¯•',
                'description': 'ä½¿ç”¨çœŸå®è§†é¢‘éªŒè¯å•ä¸ªè§†é¢‘å¤±è´¥ä¸å½±å“å…¶ä»–è§†é¢‘å¤„ç†'
            },
            {
                'name': 'çœŸå®è§†é¢‘é‡è¯•æœºåˆ¶',
                'module': 'test_batch_real_videos.py',
                'class': 'TestBatchProcessingWithRealVideos',
                'method': 'test_real_video_retry_mechanism', 
                'priority': 'HIGH',
                'category': 'é”™è¯¯æ¢å¤',
                'description': 'éªŒè¯ç½‘ç»œé”™è¯¯æ—¶çš„æ™ºèƒ½é‡è¯•å’ŒæŒ‡æ•°é€€é¿ç­–ç•¥'
            },
            {
                'name': 'çœŸå®è§†é¢‘æ–­ç‚¹ç»­ä¼ ',
                'module': 'test_batch_real_videos.py',
                'class': 'TestBatchProcessingWithRealVideos',
                'method': 'test_real_video_skip_existing',
                'priority': 'HIGH', 
                'category': 'æ–­ç‚¹ç»­ä¼ ',
                'description': 'éªŒè¯--skip-existingå‚æ•°çš„æ–­ç‚¹ç»­ä¼ åŠŸèƒ½'
            },
            {
                'name': 'çœŸå®è§†é¢‘æ€§èƒ½åŸºå‡†',
                'module': 'test_batch_real_videos.py',
                'class': 'TestBatchProcessingWithRealVideos',
                'method': 'test_real_video_performance_benchmark',
                'priority': 'CRITICAL',
                'category': 'æ€§èƒ½åŸºå‡†',
                'description': 'å»ºç«‹20ä¸ªçœŸå®Figmaè§†é¢‘çš„æ€§èƒ½å¤„ç†åŸºå‡†'
            }
        ]
        
        # æ‰§è¡Œæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
        for test_case in real_video_test_cases:
            result = self._run_single_test(test_case)
            self.test_results.append(result)
        
        self.end_time = time.time()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self._generate_comprehensive_report()
        
        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        self._print_final_summary()
        
        return report
    
    def _run_single_test(self, test_case: Dict[str, str]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªçœŸå®è§†é¢‘æµ‹è¯•ç”¨ä¾‹"""
        
        test_identifier = f"{test_case['module']}::{test_case['class']}::{test_case['method']}"
        
        print(f"\nğŸ§ª æ‰§è¡Œ: {test_case['name']}")
        print(f"   ğŸ“‹ {test_case['description']}")
        print(f"   ğŸ·ï¸  ç±»åˆ«: {test_case['category']} | ä¼˜å…ˆçº§: {test_case['priority']}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œpytestå‘½ä»¤
            result = subprocess.run([
                sys.executable, "-m", "pytest",
                f"tests/{test_identifier}",
                "-v", "--tb=short", "--no-header", "-s"  # -sæ˜¾ç¤ºprintè¾“å‡º
            ], 
            cwd=self.project_root,
            capture_output=True, 
            text=True,
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶ï¼ˆçœŸå®è§†é¢‘æµ‹è¯•å¯èƒ½è¾ƒæ…¢ï¼‰
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # è§£ææµ‹è¯•ç»“æœ
            if result.returncode == 0:
                status = "PASS"
                print(f"   âœ… é€šè¿‡ ({duration:.2f}s)")
                
                # æå–æœ‰ç”¨çš„è¾“å‡ºä¿¡æ¯
                if "çœŸå®è§†é¢‘æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡" in result.stdout:
                    performance_lines = [line.strip() for line in result.stdout.split('\n') 
                                       if line.strip().startswith(('ğŸ“Š', 'â±ï¸', 'ğŸš€', 'ğŸ“ˆ', 'ğŸ“„'))]
                    for line in performance_lines:
                        print(f"     {line}")
                        
            else:
                status = "FAIL"
                print(f"   âŒ å¤±è´¥ ({duration:.2f}s)")
                
                # æ˜¾ç¤ºå…³é”®é”™è¯¯ä¿¡æ¯
                if result.stderr:
                    error_lines = result.stderr.strip().split('\n')[-3:]  # æœ€å3è¡Œé”™è¯¯
                    for line in error_lines:
                        if line.strip():
                            print(f"   ğŸ” {line.strip()}")
            
            return {
                'test_case': test_case,
                'status': status,
                'duration_seconds': duration,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'return_code': result.returncode,
                'execution_timestamp': datetime.now().isoformat()
            }
            
        except subprocess.TimeoutExpired:
            print(f"   â±ï¸ è¶…æ—¶ (>10åˆ†é’Ÿ)")
            return {
                'test_case': test_case,
                'status': 'TIMEOUT', 
                'duration_seconds': 600,
                'stdout': '',
                'stderr': 'Test execution timeout (10 minutes)',
                'return_code': -1,
                'execution_timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            print(f"   ğŸ’¥ æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {
                'test_case': test_case,
                'status': 'ERROR',
                'duration_seconds': 0,
                'stdout': '',
                'stderr': str(e),
                'return_code': -1,
                'execution_timestamp': datetime.now().isoformat()
            }
    
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸºäºçœŸå®è§†é¢‘çš„ç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        
        total_duration = self.end_time - self.start_time
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['status'] == 'PASS'])
        failed_tests = len([r for r in self.test_results if r['status'] == 'FAIL'])
        error_tests = len([r for r in self.test_results if r['status'] in ['ERROR', 'TIMEOUT']])
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†æ
        critical_tests = [r for r in self.test_results if r['test_case']['priority'] == 'CRITICAL']
        high_tests = [r for r in self.test_results if r['test_case']['priority'] == 'HIGH']
        
        critical_passed = len([r for r in critical_tests if r['status'] == 'PASS'])
        high_passed = len([r for r in high_tests if r['status'] == 'PASS'])
        
        # æ„å»ºç»¼åˆæŠ¥å‘Š
        report = {
            'real_video_qa_execution_report': {
                'metadata': {
                    'agent_role': '@qa.mdc',
                    'test_suite': 'real_figma_videos_batch_processing_qa',
                    'video_source': 'test_videos/ - 20ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘',
                    'execution_timestamp': datetime.now().isoformat(),
                    'total_duration_seconds': total_duration,
                    'project_root': str(self.project_root),
                    'test_videos_directory': str(self.test_videos_dir)
                },
                'executive_summary': {
                    'total_tests_executed': total_tests,
                    'overall_pass_rate': passed_tests / total_tests if total_tests > 0 else 0,
                    'critical_tests_status': f"{critical_passed}/{len(critical_tests)} CRITICAL tests passed",
                    'high_priority_status': f"{high_passed}/{len(high_tests)} HIGH priority tests passed",
                    'overall_verdict': 'PASS' if failed_tests == 0 and error_tests == 0 else 'FAIL',
                    'ready_for_production': failed_tests == 0 and critical_passed == len(critical_tests)
                },
                'detailed_results': {
                    'passed': passed_tests,
                    'failed': failed_tests,
                    'errors_timeouts': error_tests,
                    'total_execution_time_minutes': total_duration / 60,
                    'average_test_duration_seconds': total_duration / total_tests if total_tests > 0 else 0
                },
                'test_breakdown_by_category': self._analyze_by_category(),
                'performance_insights': self._extract_performance_insights(),
                'test_execution_details': self.test_results,
                'quality_assessment': self._generate_quality_assessment(),
                'recommendations': self._generate_real_video_recommendations()
            }
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.project_root / "tests" / f"real_video_qa_comprehensive_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ç»¼åˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def _analyze_by_category(self) -> Dict[str, Dict[str, Any]]:
        """æŒ‰æµ‹è¯•ç±»åˆ«åˆ†æç»“æœ"""
        categories = {}
        
        for result in self.test_results:
            category = result['test_case']['category']
            if category not in categories:
                categories[category] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'avg_duration': 0,
                    'tests': []
                }
            
            categories[category]['total'] += 1
            categories[category]['tests'].append(result['test_case']['name'])
            
            if result['status'] == 'PASS':
                categories[category]['passed'] += 1
            elif result['status'] == 'FAIL':
                categories[category]['failed'] += 1
            
            categories[category]['avg_duration'] += result['duration_seconds']
        
        # è®¡ç®—å¹³å‡æ—¶é•¿
        for category in categories:
            if categories[category]['total'] > 0:
                categories[category]['avg_duration'] /= categories[category]['total']
                categories[category]['pass_rate'] = categories[category]['passed'] / categories[category]['total']
        
        return categories
    
    def _extract_performance_insights(self) -> Dict[str, Any]:
        """æå–æ€§èƒ½æµ‹è¯•æ´å¯Ÿ"""
        performance_results = [r for r in self.test_results 
                             if 'performance' in r['test_case']['name'].lower()]
        
        if not performance_results:
            return {'status': 'No performance tests executed'}
        
        insights = {}
        for result in performance_results:
            if result['status'] == 'PASS' and 'ååé‡:' in result['stdout']:
                # å°è¯•ä»è¾“å‡ºä¸­æå–æ€§èƒ½æ•°æ®
                lines = result['stdout'].split('\n')
                for line in lines:
                    if 'ååé‡:' in line:
                        insights['throughput_info'] = line.strip()
                    elif 'æ€»å¤„ç†æ—¶é—´:' in line:
                        insights['total_duration_info'] = line.strip()
                    elif 'å¹³å‡æ¯è§†é¢‘:' in line:
                        insights['avg_per_video_info'] = line.strip()
        
        return insights
    
    def _generate_quality_assessment(self) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['status'] == 'PASS'])
        
        # å…³é”®åŠŸèƒ½æµ‹è¯•çŠ¶æ€
        error_isolation_passed = any(
            r['status'] == 'PASS' and 'error_isolation' in r['test_case']['method']
            for r in self.test_results
        )
        
        retry_mechanism_passed = any(
            r['status'] == 'PASS' and 'retry_mechanism' in r['test_case']['method']
            for r in self.test_results
        )
        
        performance_benchmark_passed = any(
            r['status'] == 'PASS' and 'performance_benchmark' in r['test_case']['method']
            for r in self.test_results
        )
        
        # è´¨é‡è¯„åˆ† (0-100)
        quality_score = 0
        if passed_tests == total_tests:
            quality_score += 50  # åŸºç¡€é€šè¿‡åˆ†
        else:
            quality_score += int(50 * passed_tests / total_tests)
        
        if error_isolation_passed:
            quality_score += 20  # é”™è¯¯éš”ç¦»
        if retry_mechanism_passed:
            quality_score += 15  # é‡è¯•æœºåˆ¶
        if performance_benchmark_passed:
            quality_score += 15  # æ€§èƒ½åŸºå‡†
        
        return {
            'overall_quality_score': quality_score,
            'quality_grade': 'A' if quality_score >= 90 else 'B' if quality_score >= 80 else 'C' if quality_score >= 70 else 'D',
            'core_functionality_status': {
                'error_isolation': 'PASS' if error_isolation_passed else 'FAIL',
                'retry_mechanism': 'PASS' if retry_mechanism_passed else 'FAIL', 
                'performance_benchmark': 'PASS' if performance_benchmark_passed else 'FAIL'
            },
            'production_readiness': quality_score >= 85,
            'areas_of_concern': [
                r['test_case']['name'] for r in self.test_results 
                if r['status'] != 'PASS'
            ]
        }
    
    def _generate_real_video_recommendations(self) -> List[str]:
        """åŸºäºçœŸå®è§†é¢‘æµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        failed_tests = [r for r in self.test_results if r['status'] != 'PASS']
        
        if not failed_tests:
            recommendations.append("ğŸ‰ æ‰€æœ‰çœŸå®è§†é¢‘æµ‹è¯•é€šè¿‡ï¼æ‰¹é‡å¤„ç†åŠŸèƒ½å®Œå…¨è¾¾æ ‡")
            recommendations.append("âœ… å¯ä»¥å®‰å…¨åœ°ç”¨äºå¤„ç†çœŸå®çš„Figmaæ•™ç¨‹è§†é¢‘")
            recommendations.append("ğŸš€ å»ºè®®è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²")
        else:
            recommendations.append("âš ï¸ å‘ç°çœŸå®è§†é¢‘æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤")
            
            for failed_test in failed_tests:
                test_name = failed_test['test_case']['name']
                category = failed_test['test_case']['category']
                recommendations.append(f"ğŸ” ä¿®å¤ {category} ç±»åˆ«çš„ '{test_name}' æµ‹è¯•")
        
        # æ€§èƒ½ç›¸å…³å»ºè®®
        performance_tests = [r for r in self.test_results if 'performance' in r['test_case']['name'].lower()]
        if performance_tests:
            perf_test = performance_tests[0]
            if perf_test['duration_seconds'] > 300:  # è¶…è¿‡5åˆ†é’Ÿ
                recommendations.append("âš¡ è€ƒè™‘ä¼˜åŒ–æ‰¹é‡å¤„ç†æ€§èƒ½ï¼Œå½“å‰æµ‹è¯•è€—æ—¶è¾ƒé•¿")
        
        # åŸºäº20ä¸ªè§†é¢‘æ–‡ä»¶çš„å»ºè®®
        recommendations.append("ğŸ“Š å½“å‰åŸºå‡†åŸºäº20ä¸ªçœŸå®Figmaæ•™ç¨‹è§†é¢‘ï¼Œå¯æ‰©å±•åˆ°æ›´å¤§æ‰¹é‡")
        recommendations.append("ğŸ¯ å»ºè®®åœ¨çœŸå®APIç¯å¢ƒä¸‹è¿›è¡Œæœ€ç»ˆéªŒæ”¶æµ‹è¯•")
        
        return recommendations
    
    def _print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ¬ åŸºäºçœŸå®è§†é¢‘çš„QAæµ‹è¯•æ‰§è¡Œæ€»ç»“")
        print("=" * 70)
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['status'] == 'PASS'])
        failed_tests = len([r for r in self.test_results if r['status'] == 'FAIL'])
        
        print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»è®¡: {total_tests} ä¸ªæµ‹è¯•")
        print(f"   âœ… é€šè¿‡: {passed_tests}")
        print(f"   âŒ å¤±è´¥: {failed_tests}")
        print(f"   ğŸ“Š é€šè¿‡ç‡: {passed_tests/total_tests:.1%}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {self.end_time - self.start_time:.1f}ç§’")
        
        # å…³é”®æµ‹è¯•çŠ¶æ€
        critical_tests = [r for r in self.test_results if r['test_case']['priority'] == 'CRITICAL']
        critical_passed = len([r for r in critical_tests if r['status'] == 'PASS'])
        
        print(f"\nğŸ¯ å…³é”®æµ‹è¯•çŠ¶æ€:")
        print(f"   CRITICAL: {critical_passed}/{len(critical_tests)} é€šè¿‡")
        
        # æœ€ç»ˆåˆ¤å®š
        if failed_tests == 0:
            print(f"\nğŸ‰ QAéªŒè¯å®Œæˆï¼åŸºäº20ä¸ªçœŸå®Figmaè§†é¢‘çš„æ‰¹é‡å¤„ç†åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡")
            print(f"âœ… åŠŸèƒ½è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥äº¤ä»˜ç»™ç”¨æˆ·ä½¿ç”¨")
        else:
            print(f"\nâš ï¸ å‘ç° {failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦å¼€å‘å›¢é˜Ÿä¿®å¤åé‡æ–°éªŒè¯")
    
    def run_quick_validation(self):
        """å¿«é€ŸéªŒè¯çœŸå®è§†é¢‘æ–‡ä»¶å’Œæ ¸å¿ƒåŠŸèƒ½"""
        print("ğŸ¬ æ‰§è¡ŒçœŸå®è§†é¢‘å¿«é€ŸéªŒè¯...")
        
        # åªæ‰§è¡Œæœ€å…³é”®çš„éªŒè¯
        validation_test = 'test_batch_real_videos.py::TestBatchProcessingWithRealVideos::test_real_video_files_validation'
        
        print(f"ğŸ§ª éªŒè¯: çœŸå®è§†é¢‘æ–‡ä»¶å®Œæ•´æ€§")
        result = subprocess.run([
            sys.executable, "-m", "pytest", f"tests/{validation_test}", "-v", "-s"
        ], cwd=self.project_root)
        
        if result.returncode == 0:
            print("âœ… çœŸå®è§†é¢‘æ–‡ä»¶éªŒè¯é€šè¿‡ - å¯ä»¥è¿›è¡Œå®Œæ•´QAæµ‹è¯•")
            return True
        else:
            print("âŒ çœŸå®è§†é¢‘æ–‡ä»¶éªŒè¯å¤±è´¥ - è¯·æ£€æŸ¥test_videosç›®å½•")
            return False


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    runner = RealVideoQATestRunner()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        # å¿«é€ŸéªŒè¯æ¨¡å¼
        return runner.run_quick_validation()
    else:
        # å®Œæ•´æµ‹è¯•å¥—ä»¶
        runner.run_comprehensive_qa_suite()
        return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
